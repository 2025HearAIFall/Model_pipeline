# inference.py (GRU í˜¸í™˜ v3 ë²„ì „)
# -----------------------------------------------------------------------------
# [ë³€ê²½ ì‚¬í•­]
# 1. ëª¨ë¸ ê²½ë¡œ v3ë¡œ ë³€ê²½
# 2. onnx_predict í•¨ìˆ˜: Cell state ì œê±° (GRU í˜¸í™˜)
# -----------------------------------------------------------------------------

import os
import json
import pickle
import time
from collections import Counter
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import librosa
import onnxruntime as ort
import whisper
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# --- 0. ì„¤ì • ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# [ëª¨ë¸ A ê²½ë¡œ ìˆ˜ì •] v3 (Bi-GRU) íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½
ENCODER_PATH_A = os.path.join(BASE_DIR, 'model_a_v3_encoder.onnx')
DECODER_PATH_A = os.path.join(BASE_DIR, 'model_a_v3_decoder.onnx')
VOCAB_PATH_A   = os.path.join(BASE_DIR, 'vocab.pkl')

# (ëª¨ë¸ B)
MODEL_B_PATH_OR_NAME = "." # í˜¹ì€ huggingface ëª¨ë¸ëª…

# (ëª¨ë¸ C: ê°ì •)
MODEL_C_ONNX_PATH = os.path.join(BASE_DIR, 'cnn_gru_attn_10mb.onnx')
MODEL_C_PT_PATH   = os.path.join(BASE_DIR, 'cnn_gru_attn_10mb.pt')
EMOTION_CLASSES = ['anger','disgust','fear','happiness','neutral','sadness','surprise']
IDX2EMO = {i: c for i, c in enumerate(EMOTION_CLASSES)}

# --- ìœ í‹¸ ---
def softmax_np(z: np.ndarray) -> np.ndarray:
    z = z - z.max()
    e = np.exp(z, dtype=np.float32)
    return e / e.sum()

# -----------------------------------------------------------------------------
# 1. Vocab í´ë˜ìŠ¤ (Pickle ë¡œë”©ìš©)
# -----------------------------------------------------------------------------
class Vocabulary:
    def __init__(self, tokenizer=None, min_freq=2):
        self.tokenizer = tokenizer
        self.itos = {0: "<PAD>", 1: "<SOS>", 2: "<EOS>", 3: "<UNK>"}
        self.stoi = {v: k for k, v in self.itos.items()}
        self.min_freq = min_freq
        self.pad_idx = 0
        self.sos_idx = 1
        self.eos_idx = 2
        self.unk_idx = 3

    def __len__(self): return len(self.itos)

def simple_tokenizer(text): return text.split(' ')

# -----------------------------------------------------------------------------
# 2. ëª¨ë¸ A (ìˆ˜ì–´) ë¡œë“œ
# -----------------------------------------------------------------------------
print("ğŸ”„ ëª¨ë¸ A (ìˆ˜ì–´) ë¡œë”© ì¤‘...")
try:
    with open(VOCAB_PATH_A, 'rb') as f:
        vocab = pickle.load(f)
    
    encoder_session = ort.InferenceSession(ENCODER_PATH_A, providers=['CPUExecutionProvider'])
    decoder_session = ort.InferenceSession(DECODER_PATH_A, providers=['CPUExecutionProvider'])
    print("âœ… ëª¨ë¸ A (ONNX GRU) ë¡œë“œ ì™„ë£Œ.")
except Exception as e:
    print(f"âŒ ëª¨ë¸ A ë¡œë“œ ì‹¤íŒ¨: {e}")
    vocab, encoder_session, decoder_session = None, None, None

# ğŸ”¥ [í•µì‹¬ ìˆ˜ì •] GRUìš© ì˜ˆì¸¡ í•¨ìˆ˜ (Cell State ì œê±°)
def onnx_predict(encoder_sess, decoder_sess, src_seq_np, max_output_len, sos_idx, eos_idx):
    try:
        # 1. Encoder ì‹¤í–‰
        encoder_inputs = {'input_keypoints': src_seq_np}
        # GRU EncoderëŠ” hiddenë§Œ ë°˜í™˜ (cell ì—†ìŒ)
        encoder_outputs, hidden = encoder_sess.run(None, encoder_inputs)
        
        # 2. Decoder ì¤€ë¹„
        trg_input = np.array([sos_idx], dtype=np.int64) # (1,)
        output_tokens = []
        
        for _ in range(max_output_len):
            decoder_inputs = {
                'input_token': trg_input,
                'in_hidden': hidden,
                'encoder_outputs': encoder_outputs
            }
            
            # GRU DecoderëŠ” hiddenë§Œ ë°˜í™˜
            logits, hidden = decoder_sess.run(None, decoder_inputs)
            
            # ë‹¤ìŒ í† í° ì˜ˆì¸¡
            top1_item = int(np.argmax(logits, axis=1)[0])
            
            if top1_item == eos_idx:
                break
                
            output_tokens.append(top1_item)
            trg_input = np.array([top1_item], dtype=np.int64)
            
        return output_tokens
        
    except Exception as e:
        print(f"âš ï¸ ì˜ˆì¸¡ ì¤‘ ì—ëŸ¬: {e}")
        return []

# -----------------------------------------------------------------------------
# 3. ëª¨ë¸ B (ë¬¸ë§¥ ë³µì›) ë¡œë“œ
# -----------------------------------------------------------------------------
print("ğŸ”„ ëª¨ë¸ B (ë¬¸ë§¥) ë¡œë”© ì¤‘...")
try:
    gec_tokenizer = AutoTokenizer.from_pretrained(MODEL_B_PATH_OR_NAME)
    gec_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_B_PATH_OR_NAME).to(DEVICE)
    print("âœ… ëª¨ë¸ B ë¡œë“œ ì™„ë£Œ.")
except:
    print("âš ï¸ ëª¨ë¸ B ë¡œë“œ ì‹¤íŒ¨ (ê±´ë„ˆëœ€)")
    gec_model, gec_tokenizer = None, None

# -----------------------------------------------------------------------------
# 4. ëª¨ë¸ C (ê°ì •) ë¡œë“œ
# -----------------------------------------------------------------------------
# ... (ì‚¬ìš©ìë‹˜ì˜ ê¸°ì¡´ ê°ì • ëª¨ë¸ í´ë˜ìŠ¤ ì½”ë“œ ìœ ì§€ - ê¸¸ì´ ê´€ê³„ìƒ í•µì‹¬ë§Œ í¬í•¨) ...
class EmotionInfer:
    def __init__(self):
        self.sess = None
        if os.path.exists(MODEL_C_ONNX_PATH):
            self.sess = ort.InferenceSession(MODEL_C_ONNX_PATH, providers=['CPUExecutionProvider'])
            self.in_name = self.sess.get_inputs()[0].name
            self.out_name = self.sess.get_outputs()[0].name
            print("âœ… ëª¨ë¸ C (ê°ì • ONNX) ë¡œë“œ ì™„ë£Œ.")
        else:
            print("âš ï¸ ëª¨ë¸ C ì—†ìŒ.")

    def infer_from_file(self, audio_path):
        if not self.sess: return "Unknown", 0.0, None
        try:
            y, sr = librosa.load(audio_path, sr=16000)
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40, n_fft=2048, hop_length=512)
            mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-8)
            # Padding/Crop to 128
            if mfcc.shape[1] < 128:
                pad = np.zeros((40, 128 - mfcc.shape[1]))
                mfcc = np.hstack((mfcc, pad))
            else:
                mfcc = mfcc[:, :128]
            
            x = mfcc[None, None, :, :].astype(np.float32)
            logits = self.sess.run([self.out_name], {self.in_name: x})[0][0]
            probs = softmax_np(logits)
            idx = int(probs.argmax())
            return IDX2EMO[idx], float(probs[idx]), probs
        except Exception as e:
            print(f"Emo Error: {e}")
            return "Error", 0.0, None

emo = EmotionInfer()

# -----------------------------------------------------------------------------
# 5. ëª¨ë¸ D (STT) ë¡œë“œ
# -----------------------------------------------------------------------------
print("ğŸ”„ ëª¨ë¸ D (STT) ë¡œë”© ì¤‘...")
try:
    stt_model = whisper.load_model("base", device=DEVICE)
    print("âœ… ëª¨ë¸ D ë¡œë“œ ì™„ë£Œ.")
except:
    print("âš ï¸ ëª¨ë¸ D ë¡œë“œ ì‹¤íŒ¨.")
    stt_model = None